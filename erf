import feedparser
from datetime import datetime
from db_setup import ScrapedArticle, Session
from config import RSS_URLS
from newspaper import Article

def fetch_articles(category):
    session = Session()
    new_count = 0

    for url in RSS_URLS.get(category, []):
        feed = feedparser.parse(url)

        for entry in feed.entries[:3]:
            if session.query(ScrapedArticle).filter_by(url=entry.link).first():
                continue

            try:
                article = Article(entry.link)
                article.download()
                article.parse()

                published = article.publish_date or (
                    datetime(*entry.published_parsed[:6]) if hasattr(entry, 'published_parsed') else None
                )

                new_article = ScrapedArticle(
                    category=category,
                    title=article.title or entry.title,
                    url=entry.link,
                    summary=(article.text[:500] if article.text else entry.get('summary', '')[:500]),
                    published_at=published
                )
                session.add(new_article)
                new_count += 1

            except Exception as e:
                print(f"‚ùå Failed to parse article: {entry.link}\nError: {e}")

    session.commit()
    session.close()
    print(f"‚úÖ {category}: {new_count} new items scraped.")






from db_setup import ScrapedArticle, ProcessedArticle, Session
from datetime import datetime, timedelta
from llm_utils import summarize_text
from newspaper import Article

def deduplicate_and_process():
    session = Session()
    one_week = datetime.utcnow() - timedelta(days=7)

    for category, in session.query(ScrapedArticle.category).distinct():
        for art in session.query(ScrapedArticle).filter_by(category=category).all():
            if session.query(ProcessedArticle).filter(
                ProcessedArticle.url == art.url,
                ProcessedArticle.scraped_at >= one_week
            ).first():
                continue

            try:
                article = Article(art.url)
                article.download()
                article.parse()
                text = article.text.strip()
                summary = summarize_text(text[:2000]) if text else art.summary
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to parse or summarize {art.url}: {e}")
                summary = art.summary

            processed = ProcessedArticle(
                category=category,
                title=art.title,
                url=art.url,
                summary=summary,
                scraped_at=art.scraped_at,
                published_at=art.published_at
            )
            session.add(processed)

    session.commit()
    session.close()
    print("‚úÖ Processed and summarized new articles.")




from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime
from sqlalchemy.orm import declarative_base, sessionmaker
from datetime import datetime

Base = declarative_base()

class ScrapedArticle(Base):
    __tablename__ = 'scraped_articles'
    id = Column(Integer, primary_key=True)
    category = Column(String, nullable=False)
    title = Column(String, nullable=False)
    url = Column(String, unique=True, nullable=False)
    summary = Column(Text)
    scraped_at = Column(DateTime, default=datetime.utcnow)
    published_at = Column(DateTime, nullable=True)

    def __repr__(self):
        return f"<ScrapedArticle(title={self.title}, url={self.url})>"

class ProcessedArticle(Base):
    __tablename__ = 'processed_articles'
    id = Column(Integer, primary_key=True)
    category = Column(String, nullable=False)
    title = Column(String, nullable=False)
    url = Column(String, unique=True, nullable=False)  # ‚úÖ Added unique constraint
    summary = Column(Text)
    scraped_at = Column(DateTime, default=datetime.utcnow)  # ‚úÖ Added default
    published_at = Column(DateTime)

    def __repr__(self):
        return f"<ProcessedArticle(title={self.title}, url={self.url})>"

engine = create_engine('sqlite:///scrap.db')
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)





import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from db_setup import ProcessedArticle, Session
from config import EMAIL_HOST, EMAIL_PORT, EMAIL_ADDRESS, EMAIL_PASSWORD, RECIPIENTS

CATEGORY_NAMES = {
    "CurrentTrends": "üîç Current Trends",
    "LearnTechnologies": "üìö Learn Technologies",
    "NewTools": "üõ†Ô∏è New Tools",
    "UseCases": "üí° Use Cases"
}

def send_summary_email():
    session = Session()
    msg = MIMEMultipart("alternative")
    msg["Subject"] = "üì∞ Weekly AI Digest"
    msg["From"] = EMAIL_ADDRESS
    msg["To"] = ", ".join(RECIPIENTS)

    html = "<h2>üì∞ Weekly AI Digest</h2>"

    for cat, display_name in CATEGORY_NAMES.items():
        html += f"<h3>{display_name}</h3>"
        items = session.query(ProcessedArticle)\
                       .filter_by(category=cat)\
                       .order_by(ProcessedArticle.scraped_at.desc())\
                       .limit(3).all()
        if items:
            for art in items:
                html += (
                    f"<p><a href='{art.url}'><strong>{art.title}</strong></a><br>"
                    f"<small>{art.summary[:300]}...</small></p>"
                )
        else:
            html += "<p><em>No articles found this week.</em></p>"

    session.close()
    msg.attach(MIMEText(html, "html"))

    try:
        with smtplib.SMTP(EMAIL_HOST, EMAIL_PORT) as s:
            s.starttls()
            s.login(EMAIL_ADDRESS, EMAIL_PASSWORD)
            s.sendmail(EMAIL_ADDRESS, RECIPIENTS, msg.as_string())
        print("‚úÖ Email sent.")
    except Exception as e:
        print(f"‚ùå Email failed: {e}")






import openai
from config import OPENAI_API_KEY, MODEL_NAME

openai.api_key = OPENAI_API_KEY

def summarize_text(text):
    text = text[:3000]  # optional: prevent too-long inputs
    try:
        response = openai.ChatCompletion.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "You are an expert summarizer. Summarize text in 3‚Äì4 lines, clearly and concisely."},
                {"role": "user", "content": f"{text}"}
            ],
            temperature=0.3,
        )
        return response.choices[0].message.content.strip()
    except openai.error.OpenAIError as e:
        return f"[Summary failed: {e}]"




# Email settings
EMAIL_HOST = 'smtp.gmail.com'
EMAIL_PORT = 587
EMAIL_ADDRESS = 'ruppeshsk2003@gmail.com'
EMAIL_PASSWORD = 'rcdm asvg afwm mkgn'  # hardcoded password

# OpenAI settings
OPENAI_API_KEY = ''
MODEL_NAME = 'gpt-3.5-turbo'

# Recipients
RECIPIENTS = ['ruppeshsk12a@gmail.com','mythilytm@gmail.com','prathibaagnes@gmail.com']

# RSS sources by category
RSS_URLS = {
    'CurrentTrends': [
        'https://techcrunch.com/tag/artificial-intelligence/feed/',
        'https://www.technologyreview.com/feed/',
    ],
    'LearnTechnologies': [
        'https://openai.com/blog/rss/',
        'https://deepmind.com/blog/rss.xml',
    ],
    'NewTools': [
        'https://huggingface.co/blog/feed',
    ],
    'UseCases': [
        'https://www.ibm.com/blogs/research/feed/',
    ],
}

import time
import schedule
import logging
from datetime import datetime, timedelta
from scraper import fetch_articles
from processor import deduplicate_and_process
from emailer import send_summary_email
from db_setup import ScrapedArticle, ProcessedArticle, Session

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

def full_pipeline():
    try:
        logging.info("‚è∞ Pipeline start")
        for cat in ["CurrentTrends", "LearnTechnologies", "NewTools", "UseCases"]:
            fetch_articles(cat)
        deduplicate_and_process()
        send_summary_email()
        logging.info("üìß Email sent!")
    except Exception as e:
        logging.error(f"Pipeline error: {e}")

def cleanup():
    try:
        session = Session()
        cutoff = datetime.utcnow() - timedelta(days=7)
        session.query(ScrapedArticle).filter(ScrapedArticle.scraped_at < cutoff).delete()
        session.query(ProcessedArticle).filter(ProcessedArticle.scraped_at < cutoff).delete()
        session.commit()
        session.close()
        logging.info("‚úÖ Weekly cleanup done.")
    except Exception as e:
        logging.error(f"Cleanup error: {e}")

# === IMMEDIATE RUN TO SEND MAIL NOW ===
full_pipeline()

# === SCHEDULED RUNS ===
schedule.every().day.at("20:50").do(full_pipeline)
schedule.every().sunday.at("23:00").do(cleanup)

logging.info("üìÖ Scheduler running...")

try:
    while True:
        schedule.run_pending()
        time.sleep(60)
except KeyboardInterrupt:
    logging.info("üõë Scheduler stopped.")
