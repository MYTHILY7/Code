import feedparser
from datetime import datetime
from db_setup import ScrapedArticle, Session
from config import RSS_URLS
from newspaper import Article

def fetch_articles(category):
    session = Session()
    new_count = 0

    for url in RSS_URLS.get(category, []):
        feed = feedparser.parse(url)

        for entry in feed.entries[:3]:
            if session.query(ScrapedArticle).filter_by(url=entry.link).first():
                continue

            try:
                article = Article(entry.link)
                article.download()
                article.parse()

                published = article.publish_date or (
                    datetime(*entry.published_parsed[:6]) if hasattr(entry, 'published_parsed') else None
                )

                new_article = ScrapedArticle(
                    category=category,
                    title=article.title or entry.title,
                    url=entry.link,
                    summary=(article.text[:500] if article.text else entry.get('summary', '')[:500]),
                    published_at=published
                )
                session.add(new_article)
                new_count += 1

            except Exception as e:
                print(f"❌ Failed to parse article: {entry.link}\nError: {e}")

    session.commit()
    session.close()
    print(f"✅ {category}: {new_count} new items scraped.")






from db_setup import ScrapedArticle, ProcessedArticle, Session
from datetime import datetime, timedelta
from llm_utils import summarize_text
from newspaper import Article

def deduplicate_and_process():
    session = Session()
    one_week = datetime.utcnow() - timedelta(days=7)

    for category, in session.query(ScrapedArticle.category).distinct():
        for art in session.query(ScrapedArticle).filter_by(category=category).all():
            if session.query(ProcessedArticle).filter(
                ProcessedArticle.url == art.url,
                ProcessedArticle.scraped_at >= one_week
            ).first():
                continue

            try:
                article = Article(art.url)
                article.download()
                article.parse()
                text = article.text.strip()
                summary = summarize_text(text[:2000]) if text else art.summary
            except Exception as e:
                print(f"⚠️ Failed to parse or summarize {art.url}: {e}")
                summary = art.summary

            processed = ProcessedArticle(
                category=category,
                title=art.title,
                url=art.url,
                summary=summary,
                scraped_at=art.scraped_at,
                published_at=art.published_at
            )
            session.add(processed)

    session.commit()
    session.close()
    print("✅ Processed and summarized new articles.")
