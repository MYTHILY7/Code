import feedparser
from datetime import datetime
from db_setup import ScrapedArticle, Session
from config import RSS_URLS
from newspaper import Article

def fetch_articles(category):
    session = Session()
    new_count = 0

    for url in RSS_URLS.get(category, []):
        feed = feedparser.parse(url)

        for entry in feed.entries[:3]:
            if session.query(ScrapedArticle).filter_by(url=entry.link).first():
                continue

            try:
                article = Article(entry.link)
                article.download()
                article.parse()

                published = article.publish_date or (
                    datetime(*entry.published_parsed[:6]) if hasattr(entry, 'published_parsed') else None
                )

                new_article = ScrapedArticle(
                    category=category,
                    title=article.title or entry.title,
                    url=entry.link,
                    summary=(article.text[:500] if article.text else entry.get('summary', '')[:500]),
                    published_at=published
                )
                session.add(new_article)
                new_count += 1

            except Exception as e:
                print(f"‚ùå Failed to parse article: {entry.link}\nError: {e}")

    session.commit()
    session.close()
    print(f"‚úÖ {category}: {new_count} new items scraped.")






from db_setup import ScrapedArticle, ProcessedArticle, Session
from datetime import datetime, timedelta
from llm_utils import summarize_text
from newspaper import Article

def deduplicate_and_process():
    session = Session()
    one_week = datetime.utcnow() - timedelta(days=7)

    for category, in session.query(ScrapedArticle.category).distinct():
        for art in session.query(ScrapedArticle).filter_by(category=category).all():
            if session.query(ProcessedArticle).filter(
                ProcessedArticle.url == art.url,
                ProcessedArticle.scraped_at >= one_week
            ).first():
                continue

            try:
                article = Article(art.url)
                article.download()
                article.parse()
                text = article.text.strip()
                summary = summarize_text(text[:2000]) if text else art.summary
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to parse or summarize {art.url}: {e}")
                summary = art.summary

            processed = ProcessedArticle(
                category=category,
                title=art.title,
                url=art.url,
                summary=summary,
                scraped_at=art.scraped_at,
                published_at=art.published_at
            )
            session.add(processed)

    session.commit()
    session.close()
    print("‚úÖ Processed and summarized new articles.")




from sqlalchemy import create_engine, Column, Integer, String, Text, DateTime
from sqlalchemy.orm import declarative_base, sessionmaker
from datetime import datetime

Base = declarative_base()

class ScrapedArticle(Base):
    __tablename__ = 'scraped_articles'
    id = Column(Integer, primary_key=True)
    category = Column(String, nullable=False)
    title = Column(String, nullable=False)
    url = Column(String, unique=True, nullable=False)
    summary = Column(Text)
    scraped_at = Column(DateTime, default=datetime.utcnow)
    published_at = Column(DateTime, nullable=True)

    def __repr__(self):
        return f"<ScrapedArticle(title={self.title}, url={self.url})>"

class ProcessedArticle(Base):
    __tablename__ = 'processed_articles'
    id = Column(Integer, primary_key=True)
    category = Column(String, nullable=False)
    title = Column(String, nullable=False)
    url = Column(String, unique=True, nullable=False)  # ‚úÖ Added unique constraint
    summary = Column(Text)
    scraped_at = Column(DateTime, default=datetime.utcnow)  # ‚úÖ Added default
    published_at = Column(DateTime)

    def __repr__(self):
        return f"<ProcessedArticle(title={self.title}, url={self.url})>"

engine = create_engine('sqlite:///scrap.db')
Base.metadata.create_all(engine)
Session = sessionmaker(bind=engine)





import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from db_setup import ProcessedArticle, Session
from config import EMAIL_HOST, EMAIL_PORT, EMAIL_ADDRESS, EMAIL_PASSWORD, RECIPIENTS

CATEGORY_NAMES = {
    "CurrentTrends": "üîç Current Trends",
    "LearnTechnologies": "üìö Learn Technologies",
    "NewTools": "üõ†Ô∏è New Tools",
    "UseCases": "üí° Use Cases"
}

def send_summary_email():
    session = Session()
    msg = MIMEMultipart("alternative")
    msg["Subject"] = "üì∞ Weekly AI Digest"
    msg["From"] = EMAIL_ADDRESS
    msg["To"] = ", ".join(RECIPIENTS)

    html = "<h2>üì∞ Weekly AI Digest</h2>"

    for cat, display_name in CATEGORY_NAMES.items():
        html += f"<h3>{display_name}</h3>"
        items = session.query(ProcessedArticle)\
                       .filter_by(category=cat)\
                       .order_by(ProcessedArticle.scraped_at.desc())\
                       .limit(3).all()
        if items:
            for art in items:
                html += (
                    f"<p><a href='{art.url}'><strong>{art.title}</strong></a><br>"
                    f"<small>{art.summary[:300]}...</small></p>"
                )
        else:
            html += "<p><em>No articles found this week.</em></p>"

    session.close()
    msg.attach(MIMEText(html, "html"))

    try:
        with smtplib.SMTP(EMAIL_HOST, EMAIL_PORT) as s:
            s.starttls()
            s.login(EMAIL_ADDRESS, EMAIL_PASSWORD)
            s.sendmail(EMAIL_ADDRESS, RECIPIENTS, msg.as_string())
        print("‚úÖ Email sent.")
    except Exception as e:
        print(f"‚ùå Email failed: {e}")
