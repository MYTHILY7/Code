sources:
  - name: TechCrunch AI
    url: "https://techcrunch.com/tag/ai/feed/"
    type: rss

  - name: MIT Technology Review AI
    url: "https://www.technologyreview.com/topic/artificial-intelligence/"
    type: html

  - name: Towards Data Science
    url: "https://towardsdatascience.com/tagged/ai"
    type: html




import yaml
import feedparser
import aiohttp
from bs4 import BeautifulSoup
from datetime import datetime

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select

from database.db import SessionLocal
from models.models import RawContent

import asyncio
import os


async def save_raw_content(session: AsyncSession, source, title, url, summary, published_date, raw_html):
    result = await session.execute(select(RawContent).where(RawContent.url == url))
    existing = result.scalars().first()

    if existing:
        return  # avoid duplicates by URL

    item = RawContent(
        source=source,
        title=title,
        url=url,
        summary=summary,
        published_date=published_date,
        raw_html=raw_html,
        status="RAW"
    )
    session.add(item)
    await session.commit()


async def fetch_html(session, url):
    async with session.get(url) as response:
        return await response.text()


async def scrape_html(session: AsyncSession, source_name, url):
    async with aiohttp.ClientSession() as client:
        html = await fetch_html(client, url)
        soup = BeautifulSoup(html, "html5lib")

        # Very basic title/link grabber â€” customize this per site
        articles = soup.find_all("a")
        for a in articles:
            title = a.get_text(strip=True)
            link = a.get("href")

            if not title or not link or not link.startswith("http"):
                continue

            await save_raw_content(session, source_name, title, link, "", None, str(a))


async def scrape_rss(session: AsyncSession, source_name, url):
    feed = feedparser.parse(url)
    for entry in feed.entries:
        title = entry.get("title", "")
        link = entry.get("link", "")
        summary = entry.get("summary", "")
        published = entry.get("published", "")
        published_dt = None

        try:
            published_dt = datetime(*entry.published_parsed[:6])
        except:
            pass

        await save_raw_content(session, source_name, title, link, summary, published_dt, str(entry))


async def run_scraper():
    with open("scraper/feeds.yaml", "r") as f:
        config = yaml.safe_load(f)

    async with SessionLocal() as session:
        for source in config["sources"]:
            name = source["name"]
            url = source["url"]
            typ = source["type"]

            try:
                if typ == "rss":
                    await scrape_rss(session, name, url)
                elif typ == "html":
                    await scrape_html(session, name, url)
            except Exception as e:
                print(f"Error scraping {name}: {e}")


if __name__ == "__main__":
    asyncio.run(run_scraper())
