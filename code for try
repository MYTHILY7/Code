{
  "Current Trends on AI Technologies": [
    "https://www.artificialintelligence-news.com/ ",
    "https://techcrunch.com/tag/artificial-intelligence/ "
  ],
  "Learn About New AI Technologies": [
    "https://openai.com/news/",
    "https://research.google/research-areas/responsible-ai/ "
  ],
  "New Tools and Models on AI": [
    "https://paperswithcode.com/",
    "https://www.kdnuggets.com/tag/artificial-intelligence "
  ],
  "Use Cases on AI": [
    "https://www.cbinsights.com/research/artificial-intelligence/ ",
    "https://www.mckinsey.com/capabilities/quantumblack/our-insights "
  ]
}




import requests
from bs4 import BeautifulSoup
from newspaper import Article
import sqlite3
from urllib.parse import urljoin
from datetime import datetime
import json
import time

def load_sources():
    with open("sources.json", "r") as f:
        return json.load(f)

def init_db():
    conn = sqlite3.connect("newsletter.db")
    cursor = conn.cursor()

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS scraped_items (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            url TEXT UNIQUE,
            content TEXT,
            published_date TEXT,
            category TEXT
        )
    ''')

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS processed_items (
            id INTEGER PRIMARY KEY,
            title TEXT,
            summary TEXT,
            category TEXT
        )
    ''')

    cursor.execute('''
        CREATE TABLE IF NOT EXISTS issue_history (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            url TEXT,
            published_date TEXT,
            category TEXT
        )
    ''')

    conn.commit()
    return conn

def extract_links_from_page(page_url):
    try:
        html = requests.get(page_url, timeout=10).text
        soup = BeautifulSoup(html, 'html.parser')
        anchors = soup.find_all('a', href=True)
        links = set()
        for a in anchors:
            href = a['href']
            full_url = urljoin(page_url, href)
            if full_url.startswith('http') and '/202' in full_url:
                links.add(full_url)
        return list(links)
    except Exception as e:
        print(f"‚ùå Failed to get links from {page_url}: {e}")
        return []

def process_article(url):
    try:
        article = Article(url)
        article.download()
        article.parse()
        return {
            "title": article.title,
            "content": article.text,
            "published_date": article.publish_date.isoformat() if article.publish_date else None
        }
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to process article {url}: {e}")
        return None

def scrape():
    RSS_FEEDS = load_sources()
    conn = init_db()
    cursor = conn.cursor()

    for category, urls in RSS_FEEDS.items():
        for page_url in urls:
            print(f"üîç Scanning: {page_url}")
            article_links = extract_links_from_page(page_url)

            for article_url in article_links:
                cursor.execute("SELECT 1 FROM scraped_items WHERE url = ?", (article_url,))
                if cursor.fetchone():
                    continue

                print(f"üì∞ Processing: {article_url}")
                article_data = process_article(article_url)
                if not article_data:
                    continue

                cursor.execute('''
                    INSERT INTO scraped_items (title, url, content, published_date, category)
                    VALUES (?, ?, ?, ?, ?)
                ''', (
                    article_data["title"],
                    article_url,
                    article_data["content"],
                    article_data["published_date"] or datetime.now().isoformat(),
                    category
                ))
                conn.commit()
                print(f"‚úÖ Saved: {article_data['title'][:60]}...")
                time.sleep(1)

    conn.close()

if __name__ == "__main__":
    scrape()







import sqlite3
import openai
import os

openai.api_key = os.getenv("OPENAI_API_KEY")

def generate_summary(content):
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "Summarize this AI article."},
                {"role": "user", "content": content}
            ],
            max_tokens=300,
            temperature=0.5
        )
        return response.choices[0].message["content"].strip()
    except Exception as e:
        print(f"‚ö†Ô∏è LLM error: {e}")
        return content.strip()[:500] + "..."

def process():
    conn = sqlite3.connect("newsletter.db")
    cursor = conn.cursor()

    cursor.execute("SELECT id, title, content, category FROM scraped_items WHERE id NOT IN (SELECT id FROM processed_items)")
    rows = cursor.fetchall()

    for item_id, title, content, category in rows:
        print(f"üß† Summarizing: {title[:60]}...")
        summary = generate_summary(content)

        cursor.execute('''
            INSERT INTO processed_items (id, title, summary, category)
            VALUES (?, ?, ?, ?)
        ''', (item_id, title, summary, category))

        conn.commit()

    conn.close()
    print("üéâ Done processing.")

if __name__ == "__main__":
    process()





import sqlite3
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

SENDER_EMAIL = "you@example.com"
SENDER_PASSWORD = "your-password"
RECIPIENTS = ["user1@example.com", "user2@example.com"]

def send_newsletter():
    conn = sqlite3.connect("newsletter.db")
    cursor = conn.cursor()

    cursor.execute("SELECT title, summary, category FROM processed_items")
    items = cursor.fetchall()

    if not items:
        print("‚ö†Ô∏è Nothing to send.")
        return

    # Group by category
    sections = {}
    for title, summary, category in items:
        sections.setdefault(category, []).append((title, summary))

    html = "<h1>üß† AI Newsletter</h1>"
    for category, articles in sections.items():
        html += f"<h2>{category}</h2><ul>"
        for title, summary in articles:
            html += f"<li><strong>{title}</strong><br>{summary}</li>"
        html += "</ul>"

    msg = MIMEMultipart()
    msg["From"] = SENDER_EMAIL
    msg["To"] = ", ".join(RECIPIENTS)
    msg["Subject"] = "Your AI Newsletter"

    msg.attach(MIMEText(html, "html"))

    try:
        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(SENDER_EMAIL, SENDER_PASSWORD)
            server.send_message(msg)
        print("‚úÖ Sent successfully!")

        # Archive
        cursor.execute("SELECT title, url, published_date, category FROM scraped_items")
        for row in cursor.fetchall():
            cursor.execute("INSERT INTO issue_history (title, url, published_date, category) VALUES (?, ?, ?, ?)", row)

        cursor.execute("DELETE FROM scraped_items")
        cursor.execute("DELETE FROM processed_items")
        conn.commit()

    except Exception as e:
        print(f"‚ùå Email failed: {e}")

    conn.close()

if __name__ == "__main__":
    send_newsletter()





import schedule
import time
import os

def run_all():
    os.system("python scraper.py")
    os.system("python processor.py")
    os.system("python emailer.py")

schedule.every().monday.at("09:00").do(run_all)
schedule.every().thursday.at("09:00").do(run_all)

print("üìÖ Scheduler running... Ctrl+C to stop.")
while True:
    schedule.run_pending()
    time.sleep(60)

